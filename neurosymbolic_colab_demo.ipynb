{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Neurosymbolic Reasoner - Google Colab Demo\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChessEngineUS/neurosymbolic-reasoner/blob/main/neurosymbolic_colab_demo.ipynb)\n",
    "\n",
    "This notebook demonstrates the complete neurosymbolic AI system running on Google Colab's T4 GPU.\n",
    "\n",
    "**Features:**\n",
    "- Neural perception with transformer encoders\n",
    "- Symbolic reasoning with logic engines\n",
    "- Integrated training and inference\n",
    "- T4 GPU optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's check our GPU and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/ChessEngineUS/neurosymbolic-reasoner.git\n",
    "%cd neurosymbolic-reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch>=2.0.0 numpy scipy transformers accelerate tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import and Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_system"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from neurosymbolic import NeurosymbolicSystem, Predicate, Rule\n",
    "\n",
    "print(\"Successfully imported neurosymbolic system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_system"
   },
   "outputs": [],
   "source": [
    "# Initialize the neurosymbolic system\n",
    "print(\"Initializing neurosymbolic system...\")\n",
    "\n",
    "system = NeurosymbolicSystem(\n",
    "    input_dim=512,\n",
    "    hidden_dim=768,\n",
    "    num_concepts=50,\n",
    "    num_predicates=30\n",
    ")\n",
    "\n",
    "# Optimize for T4 GPU\n",
    "system.optimize_for_t4()\n",
    "\n",
    "print(f\"✓ System initialized on {system.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knowledge"
   },
   "source": [
    "## 3. Add Symbolic Knowledge\n",
    "\n",
    "Let's build a knowledge base with facts and rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "add_knowledge"
   },
   "outputs": [],
   "source": [
    "# Define knowledge base\n",
    "knowledge = {\n",
    "    'facts': [\n",
    "        {'name': 'mammal', 'arity': 1, 'args': ['dog']},\n",
    "        {'name': 'mammal', 'arity': 1, 'args': ['cat']},\n",
    "        {'name': 'mammal', 'arity': 1, 'args': ['whale']},\n",
    "        {'name': 'has_fur', 'arity': 1, 'args': ['dog']},\n",
    "        {'name': 'has_fur', 'arity': 1, 'args': ['cat']},\n",
    "        {'name': 'lives_in_water', 'arity': 1, 'args': ['whale']},\n",
    "        {'name': 'has_legs', 'arity': 1, 'args': ['dog']},\n",
    "        {'name': 'has_legs', 'arity': 1, 'args': ['cat']}\n",
    "    ],\n",
    "    'rules': [\n",
    "        {\n",
    "            'premises': [\n",
    "                {'name': 'mammal', 'arity': 1, 'args': ['?x']},\n",
    "                {'name': 'has_fur', 'arity': 1, 'args': ['?x']}\n",
    "            ],\n",
    "            'conclusion': {'name': 'warm_blooded', 'arity': 1, 'args': ['?x']},\n",
    "            'confidence': 0.95\n",
    "        },\n",
    "        {\n",
    "            'premises': [\n",
    "                {'name': 'has_legs', 'arity': 1, 'args': ['?x']},\n",
    "                {'name': 'has_fur', 'arity': 1, 'args': ['?x']}\n",
    "            ],\n",
    "            'conclusion': {'name': 'land_animal', 'arity': 1, 'args': ['?x']},\n",
    "            'confidence': 0.90\n",
    "        }\n",
    "    ],\n",
    "    'predicate_map': {\n",
    "        0: 'mammal',\n",
    "        1: 'has_fur',\n",
    "        2: 'warm_blooded',\n",
    "        3: 'has_legs',\n",
    "        4: 'land_animal',\n",
    "        5: 'lives_in_water'\n",
    "    }\n",
    "}\n",
    "\n",
    "system.add_knowledge(knowledge)\n",
    "print(\"✓ Knowledge base loaded\")\n",
    "print(f\"  Facts: {len(knowledge['facts'])}\")\n",
    "print(f\"  Rules: {len(knowledge['rules'])}\")\n",
    "print(f\"  Predicates: {len(knowledge['predicate_map'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perception"
   },
   "source": [
    "## 4. Neural Perception\n",
    "\n",
    "Process perceptual input through the neural module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "perception_demo"
   },
   "outputs": [],
   "source": [
    "# Generate sample perceptual input\n",
    "batch_size = 8\n",
    "seq_len = 16\n",
    "input_dim = 512\n",
    "\n",
    "print(f\"Generating input: {batch_size} samples, {seq_len} sequence length\")\n",
    "input_data = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_data = input_data.cuda()\n",
    "\n",
    "# Process through neural module\n",
    "print(\"\\nProcessing through neural encoder...\")\n",
    "with torch.no_grad():\n",
    "    neural_output = system.neural_module(input_data)\n",
    "\n",
    "print(\"✓ Neural processing complete\")\n",
    "print(f\"  Encoded shape: {neural_output['encoded'].shape}\")\n",
    "print(f\"  Concept probabilities shape: {neural_output['concept_probs'].shape}\")\n",
    "print(f\"  Top-3 activated concepts: {neural_output['concept_probs'][0].topk(3).indices.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reasoning"
   },
   "source": [
    "## 5. Symbolic Reasoning\n",
    "\n",
    "Demonstrate logical inference with the symbolic module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reasoning_demo"
   },
   "outputs": [],
   "source": [
    "# Test reasoning queries\n",
    "queries = [\n",
    "    {'name': 'warm_blooded', 'arity': 1, 'args': ['dog']},\n",
    "    {'name': 'warm_blooded', 'arity': 1, 'args': ['cat']},\n",
    "    {'name': 'land_animal', 'arity': 1, 'args': ['dog']},\n",
    "    {'name': 'mammal', 'arity': 1, 'args': ['whale']}\n",
    "]\n",
    "\n",
    "print(\"Testing reasoning queries:\\n\")\n",
    "for i, query in enumerate(queries, 1):\n",
    "    result = system.symbolic_module.reason(query, method='forward')\n",
    "    pred = Predicate(**query)\n",
    "    print(f\"{i}. Query: {pred}\")\n",
    "    print(f\"   Answer: {result['answer']}\")\n",
    "    if 'confidence' in result:\n",
    "        print(f\"   Confidence: {result['confidence']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explanation"
   },
   "outputs": [],
   "source": [
    "# Generate explanation\n",
    "query = {'name': 'warm_blooded', 'arity': 1, 'args': ['dog']}\n",
    "explanation = system.symbolic_module.explain(query)\n",
    "\n",
    "print(\"Reasoning Explanation:\")\n",
    "print(\"=\" * 50)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integrated"
   },
   "source": [
    "## 6. Integrated Perception + Reasoning\n",
    "\n",
    "Combine neural perception with symbolic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "integrated_demo"
   },
   "outputs": [],
   "source": [
    "# Run integrated system\n",
    "print(\"Running integrated neurosymbolic pipeline...\\n\")\n",
    "\n",
    "input_data = torch.randn(4, 16, 512)\n",
    "if torch.cuda.is_available():\n",
    "    input_data = input_data.cuda()\n",
    "\n",
    "query = {'name': 'warm_blooded', 'arity': 1, 'args': ['dog']}\n",
    "result = system.perceive_and_reason(input_data, query=query)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Extracted predicates: {len(result['extracted_predicates'])}\")\n",
    "\n",
    "for i, pred in enumerate(result['extracted_predicates'][:5], 1):\n",
    "    pred_name = knowledge['predicate_map'].get(pred['predicate_id'], 'unknown')\n",
    "    print(f\"{i}. {pred_name} - confidence: {pred['confidence']:.3f}\")\n",
    "\n",
    "if result['reasoning_result']:\n",
    "    print(f\"\\nReasoning Answer: {result['reasoning_result']['answer']}\")\n",
    "    if 'confidence' in result['reasoning_result']:\n",
    "        print(f\"Confidence: {result['reasoning_result']['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 7. Training Demo\n",
    "\n",
    "Quick training demonstration with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_demo"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate synthetic training data\n",
    "def generate_data(num_samples=100):\n",
    "    inputs = torch.randn(num_samples, 16, 512)\n",
    "    labels = torch.zeros(num_samples, 50)\n",
    "    for i in range(num_samples):\n",
    "        num_active = torch.randint(1, 6, (1,)).item()\n",
    "        active_concepts = torch.randperm(50)[:num_active]\n",
    "        labels[i, active_concepts] = 1.0\n",
    "    return inputs, labels\n",
    "\n",
    "print(\"Generating training data...\")\n",
    "train_inputs, train_labels = generate_data(200)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    list(system.neural_module.parameters()) + list(system.bridge.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining for 5 epochs...\")\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i in range(0, len(train_inputs), batch_size):\n",
    "        batch_inputs = train_inputs[i:i+batch_size].to(system.device)\n",
    "        batch_labels = train_labels[i:i+batch_size].to(system.device)\n",
    "        \n",
    "        labels_dict = {'concepts': batch_labels}\n",
    "        losses = system.train_step(batch_inputs, labels_dict, optimizer)\n",
    "        \n",
    "        total_loss += losses['total_loss']\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark"
   },
   "source": [
    "## 8. Performance Benchmark\n",
    "\n",
    "Measure inference speed on T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark inference speed\n",
    "batch_sizes = [4, 8, 16, 32]\n",
    "num_runs = 50\n",
    "\n",
    "print(\"Benchmarking inference speed...\\n\")\n",
    "print(f\"{'Batch Size':<12} {'Avg Time (ms)':<15} {'Throughput (samples/s)'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "system.neural_module.eval()\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        test_input = torch.randn(bs, 16, 512).to(system.device)\n",
    "        with torch.no_grad():\n",
    "            _ = system.neural_module(test_input)\n",
    "    \n",
    "    # Benchmark\n",
    "    for _ in range(num_runs):\n",
    "        test_input = torch.randn(bs, 16, 512).to(system.device)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = system.neural_module(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    throughput = bs / (avg_time / 1000)\n",
    "    \n",
    "    print(f\"{bs:<12} {avg_time:<15.2f} {throughput:.1f}\")\n",
    "\n",
    "print(\"\\n✓ Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Neural perception with transformers\n",
    "2. ✅ Symbolic reasoning with logic engines\n",
    "3. ✅ Integrated neurosymbolic inference\n",
    "4. ✅ Training pipeline\n",
    "5. ✅ T4 GPU optimization\n",
    "6. ✅ Performance benchmarking\n",
    "\n",
    "**Next Steps:**\n",
    "- Customize the knowledge base for your domain\n",
    "- Add vision encoders for image inputs\n",
    "- Integrate with NLP models for text reasoning\n",
    "- Scale up training with larger datasets\n",
    "\n",
    "**Repository:** https://github.com/ChessEngineUS/neurosymbolic-reasoner"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
